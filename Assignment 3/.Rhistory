#DB-scan
clusters_DBSCAN <- dbscan(X_train, eps = 0.2, MinPts = 9)
# Plot DBSCAN results
ggplot() +
geom_point(
aes(x = x1, y = X2, colour = as.factor(clusters_DBSCAN$cluster), fill = class, shape="."),
data = data_train, size = 1.5) +
ggtitle(paste("DBSCAN n =", n))
q()
# Assignment 2 - ST5
library(tidyverse)
library(latex2exp) # Latex in ggplot2 labels
cbPalette <- c(
"#999999", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7") # colour-blind friendly palette
# Packages for actual computation
library(MASS)  # Contains mvrnorm to simulate multivariate normal
library(class) # Classification algorithms
library(FNN)   # Fast Nearest Neighbor Search Algorithms and Applications
library(xtable)
library(smotefamily)
library(caret)
library(ggplot2)
library(randomForest)
library(tictoc)
library(dbscan)
library(rbenchmark)
library(plotly)
# data set: simulated
set.seed(7256) #Set seed to counterinteract randomness
# Simulate two very nicely separated datasets
mu1 <- matrix(c(1.3))
sig <- matrix(c(1.5))
n=250
n1 <- c(n/2,n/2)
mu <- matrix(c(-mu1[1], mu1[1], mu1[1], -mu1[1]), ncol = 2)
Sigma <- diag(c(sig, sig))
# Generate variables
X_train <- do.call(
rbind,
lapply(1:2, function(i) MASS::mvrnorm(n1[i], mu[i,], Sigma)))
y_train <- c(rep.int(0, n1[1]), rep.int(1, n1[2])) # Class labels
data_train <- tibble(
class = as.factor(y_train),
x1 = X_train[,1],
x2 = X_train[,2])
X_train
plot(X_train[,1], X_train[,2])
clusters_kmeans <- kmeans(X_train[,1:2], 2)
x1_min = -10
x1_max = 10
x2_min = -10
x2_max = 10
ggplot() +
geom_point(
aes(x = x1, y = x2, colour = as.factor(clusters_kmeans$cluster), fill = class),
data = data_train, size = 1.5) +
scale_colour_manual("Class", values = cbPalette[-1]) +
scale_fill_manual("Class", values = c(cbPalette[2], cbPalette[-1]), guide = FALSE) +
scale_x_continuous(TeX("x_1"), lim = c(x1_min, x2_max), expand = c(0, 0)) +
scale_y_continuous(TeX("x_2"), lim = c(x2_min, x2_max), expand = c(0, 0)) +
ggtitle(paste("K-means n =",n))
# data set: simulated
set.seed(7256) #Set seed to counterinteract randomness
# Simulate two very nicely separated datasets
mu1 <- matrix(c(1.3))
sig <- matrix(c(1.5))
n=50000
n1 <- c(n/2,n/2)
mu <- matrix(c(-mu1[1], mu1[1], mu1[1], -mu1[1]), ncol = 2)
Sigma <- diag(c(sig, sig))
# Generate variables
X_train <- do.call(
rbind,
lapply(1:2, function(i) MASS::mvrnorm(n1[i], mu[i,], Sigma)))
y_train <- c(rep.int(0, n1[1]), rep.int(1, n1[2])) # Class labels
data_train <- tibble(
class = as.factor(y_train),
x1 = X_train[,1],
x2 = X_train[,2])
X_train
plot(X_train[,1], X_train[,2])
clusters_kmeans <- kmeans(X_train[,1:2], 2)
x1_min = -10
x1_max = 10
x2_min = -10
x2_max = 10
ggplot() +
geom_point(
aes(x = x1, y = x2, colour = as.factor(clusters_kmeans$cluster), fill = class),
data = data_train, size = 1.5) +
scale_colour_manual("Class", values = cbPalette[-1]) +
scale_fill_manual("Class", values = c(cbPalette[2], cbPalette[-1]), guide = FALSE) +
scale_x_continuous(TeX("x_1"), lim = c(x1_min, x2_max), expand = c(0, 0)) +
scale_y_continuous(TeX("x_2"), lim = c(x2_min, x2_max), expand = c(0, 0)) +
ggtitle(paste("K-means n =",n))
# data set: simulated
set.seed(7256) #Set seed to counterinteract randomness
# Simulate two very nicely separated datasets
mu1 <- matrix(c(1.3))
sig <- matrix(c(1.5))
n=250
n1 <- c(n/2,n/2)
mu <- matrix(c(-mu1[1], mu1[1], mu1[1], -mu1[1]), ncol = 2)
Sigma <- diag(c(sig, sig))
# Generate variables
X_train <- do.call(
rbind,
lapply(1:2, function(i) MASS::mvrnorm(n1[i], mu[i,], Sigma)))
y_train <- c(rep.int(0, n1[1]), rep.int(1, n1[2])) # Class labels
data_train <- tibble(
class = as.factor(y_train),
x1 = X_train[,1],
x2 = X_train[,2])
X_train
plot(X_train[,1], X_train[,2])
dbscan::kNNdistplot(X_train, k = 5)
abline(h = 0.55, lty = 2)
#DB-scan
clusters_DBSCAN <- dbscan(X_train, eps = 0.55, MinPts = 2)
clusters_DBSCAN$clusters
# Plot DBSCAN results
ggplot() +
geom_point(
aes(x = X_train[,1], y = X_train[,2], colour = as.factor(clusters_DBSCAN$cluster), fill = class, shape="."),
data = data_train, size = 1.5) +
ggtitle(paste("DBSCAN n =",n))
# data set: simulated
set.seed(7256) #Set seed to counterinteract randomness
# Simulate two very nicely separated datasets
mu1 <- matrix(c(1.3))
sig <- matrix(c(1.5))
n=50000
n1 <- c(n/2,n/2)
mu <- matrix(c(-mu1[1], mu1[1], mu1[1], -mu1[1]), ncol = 2)
Sigma <- diag(c(sig, sig))
# Generate variables
X_train <- do.call(
rbind,
lapply(1:2, function(i) MASS::mvrnorm(n1[i], mu[i,], Sigma)))
y_train <- c(rep.int(0, n1[1]), rep.int(1, n1[2])) # Class labels
data_train <- tibble(
class = as.factor(y_train),
x1 = X_train[,1],
x2 = X_train[,2])
X_train
plot(X_train[,1], X_train[,2])
dbscan::kNNdistplot(X_train, k = 5)
abline(h = 0.1, lty = 2)
#DB-scan
clusters_DBSCAN <- dbscan(X_train, eps = 0.1, MinPts = 5)
# Plot DBSCAN results
ggplot() +
geom_point(
aes(x = X_train[,1], y = X_train[,2], colour = as.factor(clusters_DBSCAN$cluster), fill = class, shape="."),
data = data_train, size = 1.5) +
ggtitle(paste("DBSCAN n =",n))
# data set: simulated
set.seed(7256) #Set seed to counterinteract randomness
# Simulate two very nicely separated datasets
mu1 <- matrix(c(1.3))
sig <- matrix(c(1.5))
n = 0 #Sample size
j=0 #Index
nr_classes=2
time=data.frame(matrix(ncol=3,nrow=1e4)) #Data frame saving n and execution times
colnames(time) <- c("n","Kmeans","DBscan")
while (n<1e4) {
j = j+1
n=n+10 #Increase n
n1 <- c(n/2,n/2)
mu <- matrix(c(-mu1[1], mu1[1], mu1[1], -mu1[1]), ncol = 2) #Mu-matrix
Sigma <- diag(c(sig, sig)) #Standard deviations for the distributions
# Generate variables
X_train <- do.call(
rbind,
lapply(1:2, function(i) MASS::mvrnorm(n1[i], mu[i,], Sigma))) #Create train data (x1, x2)
y_train <- c(rep.int(0, n1[1]), rep.int(1, n1[2])) # Class labels
data_train <- tibble(
class = as.factor(y_train),
x1 = X_train[,1],
x2 = X_train[,2])
time$n[j] = n #Save sample size
#K-means
start_time <- Sys.time() #Start time
clusters_kmeans <- kmeans(X_train[,1:2], 2) #Kmeans
end_time <- Sys.time() #End time
time$Kmeans[j] = as.numeric(end_time - start_time) #Save time
#DB-scan
start_time <- Sys.time()
clusters_db = dbscan(X_train, eps = 0.4, MinPts = 2) #DBSCAN
end_time <- Sys.time()
time$DBscan[j] = as.numeric(end_time - start_time)
}
time = na.omit(time) #Remove NA-rows
#Plot Execution times
time$nSquared = time$n^2 #Add a column with n squared
p = ggplot(time) +
geom_point(aes((time[,1]), time[,2], color = 'K-means')) +
geom_point(aes((time[,1]), time[,3], color = 'DBSCAN')) +
ggtitle("Execution time for K-means and DBSCAN") +
xlab("n") +
ylab("time(s)") +
geom_point(aes((time[,1]), (time[,4])/3e9, colour = "(n^2)"))
p
## ----setup, include=FALSE------------------------------------------------
# General purpose packages (data handling, pretty plotting, ...)
library(tidyverse)
library(latex2exp) # Latex in ggplot2 labels
cbPalette <- c(
"#999999", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7") # colour-blind friendly palette
# Packages for actual computation
# library(MASS)    # Contains mvrnorm to simulate multivariate normal
#                  # Use as MASS::mvrnorm is recommended since MASS::select
#                  # clashes with dplyr::select
# library(RCurl)   # Helps to retrieve online datasets
# library(cluster) # Provides clustering algorithms and tools for analysis
#                  # (e.g silhouette width)
library(dendextend)  # Many more possibilities for the customization of
# dendrograms
## ----kmeans-spherical, fig.width=3, fig.height = 3.2, fig.align="center", echo=FALSE, results=FALSE, warning=FALSE, message=FALSE----
# Example borrowed from http://varianceexplained.org/r/kmeans-free-lunch/
set.seed(2019)
n <- 50000
c1 <- tibble(x = rnorm(n), y = rnorm(n), cluster = 1)
c2 <- tibble(r = rnorm(n, 5, .25), theta = runif(n, 0, 2 * pi),
x = r * cos(theta), y = r * sin(theta), cluster = 2) %>%
select(x, y, cluster)
data_plot <- bind_rows(c1, c2) %>%
mutate(cluster = as.factor(cluster))
data_plot
p1 <- ggplot(
as_tibble(data_plot %>% select(x, y) %>% scale),
aes(x = x, y = y)) +
geom_point(size = 0.5) +
theme_minimal() +
theme(
plot.title = element_text(size = 7),
axis.text = element_text(size = 7),
axis.title = element_text(size = 7)) +
ggtitle("Simulated")
p1
# Cluster on data as is
clust <- kmeans(scale(data_plot %>% select(x, y)), 2)
data_res_plot <- bind_cols(data_plot, tibble(pred = as.factor(clust$cluster)))
p2 <- ggplot(data_res_plot, aes(x = x, y = y)) +
geom_point(aes(colour = pred), size = 0.5) +
scale_colour_manual(values = cbPalette[-1], guide = FALSE) +
theme_minimal() +
theme(
plot.title = element_text(size = 7),
axis.text = element_text(size = 7),
axis.title = element_text(size = 7)) +
ggtitle("k-means directly on data")
p2
# Cluster on data as is
clust1 <- dbscan(scale(data_plot %>% select(x, y)), 2)
data_res_plot1 <- bind_cols(data_plot, tibble(pred = as.factor(clust1$cluster)))
p3 <- ggplot(data_res_plot1, aes(x = x, y = y)) +
geom_point(aes(colour = pred), size = 0.5) +
scale_colour_manual(values = cbPalette[-1], guide = FALSE) +
theme_minimal() +
theme(
plot.title = element_text(size = 7),
axis.text = element_text(size = 7),
axis.title = element_text(size = 7)) +
ggtitle("dbscan")
p3
### MVE155 Statistical Inference
### Assignment 1
### Devosmita Chatterjee
setwd("C:\Users\Acer\Desktop\MVE440\Project 3")#set current directory
read.table("breast-cancer-wisconsin.data", fileEncoding="UTF-16", dec=",")
### Assignment 3
### Devosmita Chatterjee
setwd("C:\Users\Acer\Desktop\MVE440\Project 3")#set current directory
read.table("breast-cancer-wisconsin.data", fileEncoding="UTF-16", dec=",")
### Assignment 3
### Devosmita Chatterjee
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")#set current directory
read.table("breast-cancer-wisconsin.data", fileEncoding="UTF-16", dec=",")
### Assignment 3
### Devosmita Chatterjee
f <-file("C:/Users/Acer/Desktop/MVE440/Project 3/breast-cancer-wisconsin.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
### Assignment 3
### Devosmita Chatterjee
f <-file("C:/Users/Acer/Desktop/MVE440/Project 3/breast-cancer-wisconsin.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
### Assignment 3
### Devosmita Chatterjee
f <-file("C:/Users/Acer/Desktop/MVE440/Project 3/breast-cancer-wisconsin.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
breast.cancer.wisconsin <- read.csv("C:/Users/Acer/Desktop/MVE440/Project 3/breast-cancer-wisconsin.data", header=FALSE)
View(breast.cancer.wisconsin)
head(data)
#set current directory
setwd("D:/M.SC. prog. in Engineering Mathematics and Computational Science/MiniAnalysis2")
getwd()
housedata=read.csv("kc_house_data.csv")
head(housedata)
head(f)
### Devosmita Chatterjee
f <-file("C:/Users/Acer/Desktop/MVE440/Project 3/breast-cancer-wisconsin.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
head(f)
head(data)
#set current directory
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")
getwd()
data=read.csv("data")
head(data)
ii<-sample(seq(1,dim(data)[1]),300)  # obtain a random sample of 500 indeces
train<-data[ii,]                    # select a random subset of your full dataset
row.names(train)<-seq(1,300)             # assign new IDs to each row
head(train)
dim(train)
names(train)
#set current directory
setwd("C:\Users\Acer\Desktop\MVE440\Project 3")
getwd()
housedata=read.csv("breast-cancer_csv")
#set current directory
setwd("C:\Users\Acer\Desktop\MVE440\Project 3")
getwd()
housedata=read.csv("breast-cancer_csv")
#set current directory
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")
getwd()
housedata=read.csv("breast-cancer_csv")
#set current directory
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")
getwd()
housedata=read.csv("breast-cancer_csv.csv")
head(housedata)
breast.cancer_csv <- read.csv("C:/Users/Acer/Desktop/MVE440/Project 3/breast-cancer_csv.csv")
View(breast.cancer_csv)
### Assignment 3
### Devosmita Chatterjee
### breast-cancer_csv
#set current directory
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")
getwd()
housedata=read.csv("breast-w_csv.csv")
head(housedata)
### Assignment 3
### Devosmita Chatterjee
### breast-cancer_csv
#set current directory
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")
getwd()
data=read.csv("breast-w_csv.csv")
head(data)
set.seed(1) # set the seed
ii<-sample(seq(1,dim(data)[1]),100)  # obtain a random sample of 500 indeces
train<-data[ii,]                    # select a random subset of your full dataset
row.names(train)<-seq(1,100)             # assign new IDs to each row
head(train)
dim(train)
names(train)
# Randomly select subset of 500 observations
set.seed(1) # set the seed
ii<-sample(seq(1,dim(data)[1]),500)  # obtain a random sample of 500 indeces
train<-data[ii,]                    # select a random subset of your full dataset
row.names(train)<-seq(1,500)             # assign new IDs to each row
head(train)
dim(train)
names(train)
remainingdata<-housedata[-ii,] # letâs eliminate the observations indexed by ii
ii<-sample(seq(1,dim(remainingdata)[1]),500) # these are indeces for testing data
test<-remainingdata[ii,]
row.names(test)<-seq(501,1000)
head(test)
dim(test)
names(test)
head(train)
breast.w_csv <- read.csv("C:/Users/Acer/Desktop/MVE440/Project 3/breast-w_csv.csv")
View(breast.w_csv)
# create data frame of transformed response and predictor variables
train=data.frame(train$Clump_Thickness, train$Cell_Size_Uniformity, train$Cell_Shape_Uniformity, train$Marginal_Adhesion, train$Single_Epi_Cell_Size, train$Bare_Nuclei, train$Bland_Chromatin, train$Normal_Nucleoli, train$Mitoses)
df_names=c("Clump_Thick", "Cell_Size", "Cell_Shape", "Adhesion", "Epi_Cell_Size", "Bare_Nuclei", "Bland_Chrom", "Nucleoli", "Mitoses")
names(train)=df_names
library(GGally)
ggpairs(data=train,columns=1:9)
# a graphical summary of collinearity
library(gplots)
df=train[,1:9]
distmat=1-cor(df)
hh=heatmap.2(as.matrix(distmat),col=redgreen(75),cexROW=.5,key=TRUE,symkey=FALSE,
density.info='none',trace='none',srtRow=45,srtCol=45,
cexRow=0.75,cexCol=0.75)
# a graphical summary of collinearity
library(ggplots)
df=train[,1:9]
distmat=1-cor(df)
hh=heatmap.2(as.matrix(distmat),col=redgreen(75),cexROW=.5,key=TRUE,symkey=FALSE,
density.info='none',trace='none',srtRow=45,srtCol=45,
cexRow=0.75,cexCol=0.75)
# a graphical summary of collinearity
library(ggplot)
df=train[,1:9]
distmat=1-cor(df)
hh=heatmap.2(as.matrix(distmat),col=redgreen(75),cexROW=.5,key=TRUE,symkey=FALSE,
density.info='none',trace='none',srtRow=45,srtCol=45,
cexRow=0.75,cexCol=0.75)
install.packages("gplots")
# a graphical summary of collinearity
library(gplots)
df=train[,1:9]
distmat=1-cor(df)
hh=heatmap.2(as.matrix(distmat),col=redgreen(75),cexROW=.5,key=TRUE,symkey=FALSE,
density.info='none',trace='none',srtRow=45,srtCol=45,
cexRow=0.75,cexCol=0.75)
df
distmat=1-cor(df)
distmat
### Assignment 3
### Devosmita Chatterjee
#set current directory
setwd("C:/Users/Acer/Desktop/MVE440/Project 3")
getwd()
data=read.csv("breast-w_csv.csv")
head(data)
# Randomly select subset of 500 observations
set.seed(1) # set the seed
ii<-sample(seq(1,dim(data)[1]),500)  # obtain a random sample of 500 indeces
train<-data[ii,]                    # select a random subset of your full dataset
row.names(train)<-seq(1,500)             # assign new IDs to each row
head(train)
dim(train)
names(train)
remainingdata<-housedata[-ii,] # letâs eliminate the observations indexed by ii
ii<-sample(seq(1,dim(remainingdata)[1]),500) # these are indeces for testing data
test<-remainingdata[ii,]
row.names(test)<-seq(501,1000)
head(test)
dim(test)
names(test)
head(train)
# create data frame of transformed response and predictor variables
train=data.frame(train$Clump_Thickness, train$Cell_Size_Uniformity, train$Cell_Shape_Uniformity, train$Marginal_Adhesion, train$Single_Epi_Cell_Size, train$Bland_Chromatin, train$Normal_Nucleoli, train$Mitoses)
df_names=c("Clump_Thick", "Cell_Size", "Cell_Shape", "Adhesion", "Epi_Cell_Size", "Bland_Chrom", "Nucleoli", "Mitoses")
names(train)=df_names
library(GGally)
ggpairs(data=train,columns=1:8)
# a graphical summary of collinearity
library(gplots)
df=train[,1:8]
df
distmat=1-cor(df)
distmat
hh=heatmap.2(as.matrix(distmat),col=redgreen(75),cexROW=.5,key=TRUE,symkey=FALSE,
density.info='none',trace='none',srtRow=45,srtCol=45,
cexRow=0.75,cexCol=0.75)
library(reshape2)
setwd("C:/Users/Acer/Desktop/MVE155/Assignment3")#set current directory
getwd()
DF = read.csv('fruitfly.txt', quote='\'')
dim(DF)
DF$females = as.factor(DF$females)
DF$type = as.factor(DF$type)
type = rep('NA', dim(DF)[1])
type[DF$type==0] = 'pregnant'
type[DF$type==1] = 'virgin'
DF$type = type
DF$type = as.factor(DF$type)
DF
#excluded size
mm=lm(class~Clump_Thick+Cell_Shape+Adhesion+Epi_Cell_Size+Bland_Chrom+Nucleoli+Mitoses,data=train) # use train
summary(mm)
par(mfrow=c(2,2))
plot(mm) # diagnostic plots
# --- Model selection using backward selection
selectstep=step(mm,directions="backward",trace=F)
size_model=length(names(selectstep$coefficients)) # including intercept
params=names(selectstep$coefficients)[-1]
predval=predict(selectstep,newdata=test[,-1])
prederror=sum((test_y-predval)^2)/length(predval)
print("Backward selection pMSE:")
prederror
plot(predval,test_y, main="Observed vs. predicted for model from mini 2",xlab="Predicted",ylab="Observed")
# --- Model selection using backward selection
remainingdata<-data[-ii,] # eliminate the observations indexed by ii
ii<-sample(seq(1,dim(remainingdata)[1]),round(1000*(1-frac))) # these are indeces for testing data
test<-remainingdata[ii,]
row.names(test)<-seq(round(1000*frac)+1,1000)
test_x=as.matrix(test[,covars]) # include only pre-selected covariates
test_y=test[,1] # response variable
selectstep=step(mm,directions="backward",trace=F)
size_model=length(names(selectstep$coefficients)) # including intercept
params=names(selectstep$coefficients)[-1]
predval=predict(selectstep,newdata=test[,-1])
prederror=sum((test_y-predval)^2)/length(predval)
print("Backward selection pMSE:")
prederror
plot(predval,test_y, main="Observed vs. predicted for model from mini 2",xlab="Predicted",ylab="Observed")
# --- Model selection using backward selection
remainingdata<-data[-ii,] # eliminate the observations indexed by ii
ii<-sample(seq(1,dim(remainingdata)[1]),round(500*(1-frac))) # these are indeces for testing data
test<-remainingdata[ii,]
row.names(test)<-seq(round(500*frac)+1,500)
test_x=as.matrix(test[,covars]) # include only pre-selected covariates
test_y=test[,1] # response variable
selectstep=step(mm,directions="backward",trace=F)
size_model=length(names(selectstep$coefficients)) # including intercept
params=names(selectstep$coefficients)[-1]
predval=predict(selectstep,newdata=test[,-1])
prederror=sum((test_y-predval)^2)/length(predval)
print("Backward selection pMSE:")
prederror
plot(predval,test_y, main="Observed vs. predicted for model from mini 2",xlab="Predicted",ylab="Observed")
